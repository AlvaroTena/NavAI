import itertools
from os import path
from typing import Union

import joblib
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.compose import ColumnTransformer

from mlnav.utils.logger import Logger


class Predictor:
    """Predictor is a class that provides methods for making predictions using a KMeans model.

    Attributes:
        model (KMeans): The machine learning model to be used for prediction.
        predictions: The predicted labels generated by the model.
    """

    def __init__(self, model: KMeans) -> None:
        """Initialize a predictor instance.

        Args:
            model (KMeans):
                The machine learning model to be used for prediction.
        """
        self.model: KMeans = model
        self.predictions = None

    def get_predictions(
        self,
        prediction_data: pd.DataFrame,
        include_input_data: bool = False,
        normalized: bool = True,
        transformers_dir: str = None,
    ) -> Union[pd.DataFrame, np.ndarray]:
        """Get predictions from the model for the given prediction data.

        Args:
            prediction_data (pd.DataFrame):
                The input data for making predictions.
            include_input_data (bool, optional):
                Flag indicating whether to include input data in the returned result. Defaults to False.
            normalized (bool, optional):
                Flag indicating whether the input data is normalized. Defaults to True.
            transformers_dir (str, optional):
                Directory path where the column transformers are saved. Defaults to None.

        Returns:
            predictions : pd.DataFrame or np.ndarray
                If include_input_data is True, a DataFrame with predictions and input data. Otherwise, a
                numpy array with the predictions.
        """
        if not normalized:
            Logger.getLogger().debug(
                "Prediction data is NOT normalized, applying transformers..."
            )
            prediction_data = self.__normalize_data(prediction_data, transformers_dir)

        self.predictions = self.model.predict(
            prediction_data[self.model.feature_names_in_]
        ).astype(np.uint8)

        if include_input_data:
            prediction_data.insert(
                loc=len(prediction_data.columns),
                column="prediction",
                value=self.predictions,
            )
            return prediction_data
        else:
            return self.predictions

    def map_predictions(self, mapping_data: pd.DataFrame) -> pd.DataFrame:
        """Map predictions to mapping data.

        Args:
            mapping_data (pd.DataFrame):
                The data to which predictions need to be mapped.

        Raises:
            ValueError:
                If no predictions have been generated previously.
            ValueError:
                If the length of the mapping data does not match the length of the predictions.
            ValueError:
                If the required columns for prediction mapping are not present in the mapping data.

        Returns:
            mapping_data : pd.DataFrame
                The mapping data with the added predictions column.
        """
        if self.predictions is None:
            e = "No predictions have been previously generated."
            Logger.getLogger().error(e)
            raise ValueError(e)

        if len(mapping_data) != len(self.predictions):
            e = "The length of the mapping data does not match the length of the predictions."
            Logger.getLogger().error(e)
            raise ValueError(e)

        prediction_features = ["epoch", "cons", "sat_id", "freq"]
        if not all(column in mapping_data.columns for column in prediction_features):
            e = "The columns required for prediction mapping are not present in the mapping data."
            Logger.getLogger().error(e)
            raise ValueError(e)

        mapping_data["sat_id"] = pd.Categorical(
            mapping_data["cons"].astype(str)
            + mapping_data["sat_id"].astype(str).str.zfill(2),
            categories=[
                f"{cons}{sat_id:02d}"
                for cons, sat_id in list(itertools.product(["G", "E"], range(1, 41)))
                + list(itertools.product(["B"], range(1, 64)))
            ],
        )
        mapping_data.drop(["cons"], axis=1, inplace=True)
        mapping_data.insert(
            loc=len(mapping_data.columns),
            column="predictions",
            value=self.predictions,
        )
        return mapping_data[["epoch", "sat_id", "freq", "predictions"]]

    def __normalize_data(
        self, prediction_data: pd.DataFrame, transformers_dir: str
    ) -> pd.DataFrame:
        """Normalize the prediction data using transformers stored in the specified directory.

        Args:
            prediction_data (pd.DataFrame):
                The prediction data to be normalized.
            transformers_dir (str):
                Directory path where the column transformers are saved.

        Raises:
            ValueError: If the transformers directory is not a valid path.

        Returns:
            prediction_data : pd.DataFrame
                The normalized prediction data.
        """
        if transformers_dir is None or not path.exists(transformers_dir):
            e = "The transformers directory must be a valid path."
            Logger.getLogger().error(e)
            raise ValueError(e)

        drop_columns = []
        columns_order = []
        for column in prediction_data.columns:
            transformer_path = path.join(
                transformers_dir, f"{column}_ColumnTransformer.pkl"
            )
            if path.exists(transformer_path):
                transformer: ColumnTransformer = joblib.load(transformer_path)

                column_norm = transformer.transform(prediction_data[[column]])
                column_norm = column_norm.astype(
                    np.int8
                    if np.isclose(column_norm, np.round(column_norm), atol=1e-5).all()
                    else np.float32
                )
                if "OneHotEncoder" in str(transformer.named_transformers_):
                    out_columns = [
                        cat
                        for cat in list(transformer.named_transformers_.values())[
                            0
                        ].categories_[0]
                    ]
                    for col, norm in zip(out_columns, column_norm.transpose()):
                        prediction_data[col] = norm

                    columns_order += out_columns
                    drop_columns.append(column)
                else:
                    prediction_data[column] = column_norm
                    columns_order.append(column)

                Logger.getLogger().debug(f"{column} transformer applied.")
            else:
                drop_columns.append(column)

        prediction_data.drop(drop_columns, axis=1, inplace=True)
        return prediction_data[columns_order]
